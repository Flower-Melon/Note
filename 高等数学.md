# 读研以来对高等数学新的认识

> 本科期间的**微积分，线性代数，概率论与数理统计**可谓是最重要的三门数学课，是所有研究应该掌握的基础。但是很遗憾，得益于本科教育垃圾的教学模式，应试的考察方式和狗屎一样的教材，我这几门课和没学区别也不是很大。

> 读研以来，对几门数学的理解驻日加深，虽然不知道理解了有什么卵用，但是系统地学会一些知识总是好的，特在此整理了笔记，梳理一下我的一些关键理解

> * 首先感谢华科矩阵论的教师**崔洪勇老师**，他教授的矩阵论课程深入浅出，让我真正开始弄懂了矩阵的本质
> * 然后我还要感谢伟大的**Gemini**，和**Gemini**交谈的过程中让我顿悟了许多东西
> * 最后要感谢`bilibili`的`up`主`3Blue1Brown`和知乎优质的创作者们，你们的分享给了我大量的帮助


## 1 矩阵论

### 1.1 **矩阵是线性变换**

一切的一切首先要从**矩阵是线性变换**这个结论开始，线性空间的性质和定义这里不做赘述。

线性空间的概念是十分宽泛的，实际上我们平常处理的最多的是向量空间，除此以为还有矩阵空间，多项式空间和函数空间，而维度为2的向量空间是最经典的欧式空间。后面的讨论也以最好可视化的向量空间为例

![线性空间](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性空间.png)

向量组定义在向量空间里，是向量的聚合方式（比如神经网络中的一个`batch`构成一个向量组），而矩阵本质上是线性变换的一个算子，矩阵通过和向量相乘将向量映射成另一个向量

> 这里我还是想吐槽一下编写线性代数课本的这些傻卵，把如此成体系的知识讲的毫无逻辑可言。上来就讲行列式，课程推进的过程中矩阵，向量组，行列式交杂在一起，让我一度以为这几个东西就是一回事，一通学下来只会做几道破题，舍本逐末

以下是一个线性变换的例子，可以将以标准正交基为底的向量空间中的向量旋转 $\theta$ 角度

![旋转变换](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/旋转变换.png)

更为普遍地，由空间的基底在该线性变换下的映射可轻松得出该线性变换的矩阵：

![线性变换和矩阵](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性变换和矩阵.png)

进一步对于任意向量，通过矩阵与其坐标的相乘可以快速得到线性变换后的坐标：

![线性变换和矩阵2](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性变换和矩阵2.png)

> 如果是标准正交基，向量本身和坐标相等，直接用矩阵乘向量本身即可得到变换后的向量

### 1.2 特征值和特征向量

容易想到，矩阵作为线性变换实际上需要以基底为基础求出，当基底不同时，同一个线性变换的对应的矩阵可能不同，以下是一个例子：

设定二维空间 $\mathbb{R}^2$ 上的变换规则：

$$T\left(\begin{bmatrix} x \\ y \end{bmatrix}\right) = \begin{bmatrix} 2x \\ x+y \end{bmatrix}$$

选取标准基 $\mathcal{E} = \{e_1, e_2\}$，其中 $e_1=\begin{bmatrix} 1 \\ 0 \end{bmatrix}, e_2=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$。

$$T(e_1) = \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \mathbf{2}e_1 + \mathbf{1}e_2$$

$$T(e_2) = \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \mathbf{0}e_1 + \mathbf{1}e_2$$

提取系数，得到矩阵 **$A$**：

$$A = \begin{bmatrix} 2 & 0 \\ 1 & 1 \end{bmatrix}$$

选取新基底 $\mathcal{B} = \{v_1, v_2\}$ ，其中 $v_1=\begin{bmatrix} 1 \\ 1 \end{bmatrix}, v_2=\begin{bmatrix} 1 \\ 0 \end{bmatrix}$

$$T(v_1) = \begin{bmatrix} 2 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \mathbf{2}v_1 + \mathbf{0}v_2$$

$$T(v_2) = \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 1\begin{bmatrix} 1 \\ 1 \end{bmatrix} + 1\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \mathbf{1}v_1 + \mathbf{1}v_2$$

提取系数，得到矩阵 **$B$**：

$$B = \begin{bmatrix} 2 & 1 \\ 0 & 1 \end{bmatrix}$$

事实上，不同基底下的矩阵是相似的（从这个角度理解想似对角化比较好）：

![矩阵相似](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/矩阵相似.png)

那么其实如果只给定你一个矩阵，不告知你基底是什么，实际上我们无法直接知道关于这个线性变换的更多信息，这时候我们需要发掘矩阵的核心特质了，同一个线性变换对应的不同矩阵，有没有什么共同点呢？

$$AX = \lambda X$$

以上是特征值和特征向量的定义式，在线性代数初次见到它的时候，我对什么是特征值和什么是特征向量毫无概念，如今，我终于能深刻理解这两个概念的意义了

特征向量是变换中的方向不变的向量，特征值是被拉伸或压缩的倍数。在线性变换过程中，绝大多数向量的方向都会改变，总有那么几个特殊的向量，在变换前后，它们的方向完全没有改变，依然保持在原来的直线上。这些特殊的向量就是特征向量。

特征向量是线性变换的主轴：  

![主轴](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/主轴.png)

同一个线性变换对应的不同矩阵，特征值完全相等，特征向量的几何实体相等（坐标可能不等，但不重要）

### 1.3 行列式

在充分理解了矩阵的本质后，我们终于可以描述行列式的意义了

> 这里第三次吐槽傻卵线性代数，上来就教行列式，这能学的懂就有鬼了，还没学会走路就开始跑了，就因为行列式好计算，好出题，他妈的就是纯为了做题而生的

可以想到，经过线性变换后，整个向量空间被映射到了另一个向量空间上，如何衡量整个空间的放缩尺度，这时候行列式应运而生。行列式有着极强的几何意义，并非只是让你算着玩玩和判定一些条件的工具

行列式 $\det(A)$ 是线性变换对空间**有向体积**的伸缩倍数：

考虑在二维平面上，我们有一个标准的单位正方形，它的边由基底向量 $\vec{i}=(1,0)$ 和 $\vec{j}=(0,1)$ 围成，面积显然是 $1 \times 1 = 1$

当我们用一个矩阵 $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ 作用于这个平面时：基底 $\vec{i}$ 变成了向量 $(a, c)$ ，基底 $\vec{j}$ 变成了向量 $(b, d)$ 

原来的单位正方形，被拉扯变成了一个平行四边形。行列式的值 $\det(A) = ad - bc$，正是这个变换后的平行四边形的有向面积。

* 行列式为0

行列式为0说明经过线性变换后的空间损失了维度，故面积为0，这也意味着变换过程损失了信息，因此行列式为0是矩阵不可逆的判定条件

* 行列式的符号

**$\det(A) > 0$（正）：** **保持手性**。空间的相对方向不变（右手系依然是右手系）
**$\det(A) < 0$（负）：** **翻转手性**。空间发生了的翻转（右手系变为左手系）

* 雅可比行列式

> 在微积分的二重积分换元时，我第一次看到了雅可比行列式的概念，很遗憾，课本上直接抛出了这个概念，并未给出任何解释，我当时只是隐约知道这是一个缩放因子

$$J = \begin{pmatrix} 
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ 
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} 
\end{pmatrix} $$

雅可比矩阵描述了换元的对应关系，实质上是一个映射，很显然，这个矩阵的行列式的值描述了换元后微元的面积变化情况，所以在进行换元时要乘这个面积缩放因子

### 1.4 奇异值分解和PCA分析

上面的讨论主要基于方阵，当矩阵的行列不相等时，虽然也表示线性变换，但这个时候特征值的定义方程是 $Ax = \lambda x$。如果 $A$ 是 $m \times n$ 的矩阵（且 $m \neq n$）：输入的向量 $x$ 是 $n$ 维的。输出的向量 $Ax$ 是 $m$ 维的。这就导致 $Ax$ 和 $x$ 属于不同的空间，无法进行比较（无法说 $Ax$ 是 $x$ 的 $\lambda$ 倍）。

这时候就要使用伟大的奇异值分解了，对于任意矩阵 $A_{m \times n}$，我们可以将其分解为三个矩阵的乘积：

$$A = U \Sigma V^T$$

这里的三个部分分别对应了特征值和特征向量的推广概念：

* $\Sigma$ (奇异值矩阵)： 这是一个 $m \times n$ 的对角矩阵（通常只有主对角线上有非零值）。对角线上的元素 $\sigma_i$ 称为奇异值。对应关系： 奇异值 $\sigma$ 类似于方阵的特征值 $\lambda$ 。它们代表了变换在不同方向上的“拉伸”程度。

* $V$ (右奇异向量矩阵)： 这是一个 $n \times n$ 的正交矩阵。它的列向量 $v_i$ 称为右奇异向量。对应关系： 它们类似于原空间（输入空间）中的特征向量方向。

* $U$ (左奇异向量矩阵)： 这是一个 $m \times m$ 的正交矩阵。它的列向量 $u_i$ 称为左奇异向量。对应关系： 它们是变换后空间（输出空间）中对应的基向量。

求解来说参照以下过程：

![SVD求解](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/SVD求解.png)

实际上在求解过程中我们可以看到：

$$A{v_i} = \sigma {u_i}$$

使用两个不同维度的空间中各自的特征向量就行了，其中 ${A^H}A$ 通过来回变换的叠加，强行构造特征值（所以奇异值要求根号）

* `PCA`分析

这里顺带提一下之前机器学习中使用的主成分分析，这本质就是一个`SVD`过程，分析的结果不同而已，懒得赘述了

### 1.5 线性方程组解的判定和秩-零度定理

先重述一下线性方程组解的判定条件：

1. **齐次线性方程组 $Ax=0$**  
    - 若 $\mathrm{rank}(A) = n$（ $n$ 为未知数个数），只有零解。
    - 若 $\mathrm{rank}(A) < n$ ，有无穷多非零解。

2. **非齐次线性方程组 $Ax=b$**   
    - 若 $\mathrm{rank}(A) = \mathrm{rank}([A|b]) = n$ ，有唯一解。
    - 若 $\mathrm{rank}(A) = \mathrm{rank}([A|b]) < n$ ，有无穷多解。
    - 若 $\mathrm{rank}(A) \neq \mathrm{rank}([A|b])$ ，无解。

这里先介绍秩-零度定理：

对于一个 $m \times n$ 的矩阵 $A$（或者一个从 $n$ 维空间到 $m$ 维空间的线性变换 $T$）：

$$\text{Rank}(A) + \text{Nullity}(A) = n$$

* n 即输入的自由度
* Rank($A$) (秩)即矩阵列空间的维度，描述了线性变换后输出的维度（像空间）
* Nullity($A$) (零度)即矩阵零空间的维度

根据秩-零度定理非常好理解线性方程组解结构的判定

* 矩阵的行秩等于列秩

这个结论实际上非常不直观不显然，当时学的时候也是直接抛出了这个结论（其实是给了实例，矩阵经过行列初等变化后发现相等，在我看来非常不靠谱），我查阅了很多资料都不能理解

一方面由秩零度定理，另一方面因为矩阵的行空间是零空间的正交补子空间，故行秩等于列秩（也不算很清晰，但也可以说服自己了。也许将来可以理解的更为深刻吧）

## 2 微积分

### 2.1 `e`的本质

`e`的课本上的定义式如下：

$$e = \mathop {\lim }\limits_{n \to \infty } {\left( {1 + \frac{1}{n}} \right)^n}$$

或者可以用无穷级数定义：

$$e = \sum_{n=0}^{\infty} \frac{1}{n!} = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots$$

但这为什么那么多公式里面都有`e`，上面的定义不能给出直观的理由

在理解了特征值和特征向量后，这里有更涉及本质的解释：

首先，要把函数定义成线性空间，对于相同定义域上的连续实函数有：

$$(f+g)(x) = f(x) + g(x)$$

$$(c \cdot f)(x) = c \cdot f(x)$$

可以认为在这一个定义域上的所有函数构成一个线性空间。这里卖一个关子，这个线性空间的基底怎么写，整体函数空间也太大了（靠傅里叶变换）

定义了函数的线性空间后，实际上如果把求导看成一种算子 $D$ ，那么求导实际上是一个线性变换：

$$D\left( {f + k \cdot g} \right) = Df + k \cdot Dg$$

下面的例子展示了多项式空间中求导对应的矩阵：

![求导的矩阵](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/求导的矩阵.png)

接下来就好说了，直接给出下面的两个公式：

$$AX = \lambda X$$

$$D\left( {\lambda {e^{\lambda x}}} \right) = \lambda {e^{\lambda x}}$$

这时候我震惊的发现：**$e^{\lambda x}$ 是微分算子的特征向量**

这是`e`的本质是变化率的最直观的解释

> 很可惜，以上的两个公式是和`Gemini`闲聊时获得的，并非我自己想到的，但在看到这两个公式的一瞬间，我就瞬间意识到了它们意味着什么，大脑甚至宕机了一段时间

至于`e`是如何定义的，考虑对指数函数求导：

$$\frac{d}{{dx}}{a^x} = \mathop {\lim }\limits_{\Delta x \to 0} \frac{{{a^x}{a^{\Delta x}} - {a^x}}}{{\Delta x}} = \mathop {\lim }\limits_{\Delta x \to 0} {a^x}\frac{{{a^{\Delta x}} - 1}}{{\Delta x}}$$

如果希望 $\mathop {\lim }\limits_{\Delta x \to 0} \frac{{{a^{\Delta x}} - 1}}{{\Delta x}} = 1$

那么很显然：

$$a = \mathop {\lim }\limits_{\Delta x \to 0} {\left( {1 + \Delta x} \right)^{\frac{1}{{\Delta x}}}}$$

我们可以把这里的底数`a`定义为`e`

至于上述极限怎么求解，我认为应该有了`e`的性质以后，通过泰勒级数求解，而不是直接通过上述极限获得`e`的值

### 2.2 微分的一些认识

从求导开始，这里还是要重写一下求导的定义：

$$\mathop {\lim }\limits_{\Delta x \to 0} \frac{{f\left( {x + \Delta x} \right) - f\left( x \right)}}{{\Delta x}}$$

求导是求极限的过程，要时刻铭记这一点

* 连续不一定可导，可导一定连续

这个还是比较好理解的，回忆一下函数连续的定义：

$$\mathop {\lim }\limits_{\Delta x \to 0} f\left( {x + \Delta x} \right) = f\left( x \right)$$

容易看出导数存在是比函数连续的更强的条件

* 微分和函数变化量

![图解微分](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/图解微分.png)

$$\Delta y = 2x \cdot dx + {\left( {dx} \right)^2}$$

$$dy = 2x \cdot dx$$

从上图看出微分并不是函数变化的全部，$dy$ 只保留了和 $dx$ 变换中完全线性的部分而忽略了 $dx^2$ ，而保留的部分实际上就是导数作用的部分

这是微分处理的核心所在，**认为函数在极小的领域上关于变化量是线性变化的**，这是导数最有用的用途

* 一阶微分的形式不变性

这个结论其实没什么用且很好理解，在这里重新回顾它是因为我当初在学习的过程中对这个结论糊里糊涂的，今天直接把它给分析透了

这个结论描述的其实是复合函数的微分法则，可以直接对中间变量进行微分：

$$y = f\left( u \right),u = g\left( x \right)$$

$$dy = f'\left( u \right)du = f'\left( u \right)g'\left( x \right)dx$$

而情况到了二阶微分以后：

$${d^2}y = d\left( {f'\left( u \right)du} \right) = f''\left( u \right)d{u^2} + f'\left( u \right){d^2}u$$

这里由于 ${d^2}u \ne 0$ （实际上 ${d^2}x = 0$ ），所以 ${d^2}y \ne f''\left( u \right)d{u^2}$

> 课本上能把简单的事情说这么复杂也是真神人了，典型的不说人话

* 泰勒展开

可以想到，微分希望的是在函数某个点极小的领域内使用求 $dx$ 的线性近似，如果我们希望可以更进一步精确一些，或者我们希望这个领域就算大一点我们也可以用类似的多项式去近似函数，泰勒展开就是为此而生的

无穷级数形式：

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n$$

泰勒中值定理形式：

$$f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)$$

### 2.3 积分的一些认识

* 面积的定义
  
在开始之前首先要明确，积分来源于求解函数图像下围成的东西的面积，最终却定义了这些面积是什么

首先是一条公理：一个长为 $w$、宽为 $h$ 的矩形，其面积定义为 $A = w \times h$ 这是积分计算面积的前提

接下来黎曼对积分进行了定义，具体的说，在区间 $[a, b]$ 上具有有限个间断点的函数可以积分（充分条件，这里提一嘴迪利克雷函数，处处不连续，处处不可导，这样的函数就不能用黎曼积分做）

黎曼积分定义为：

$$\int_a^b {f\left( x \right)dx}  = \mathop {\lim }\limits_{\lambda  \to 0} \sum\limits_{i = 1}^n {f\left( {{\xi _i}} \right)\Delta {x_i}} $$

其中 $\lambda  = \max \left( {\Delta {x_i}} \right)$ ，是的，积分也是一个极限

* 牛顿-莱布尼茨公式

相较于一般形式，我更喜欢`3Blue1Brown`给出的形式：

$$\frac{{\int_a^b {f\left( x \right)dx} }}{{b - a}} = \frac{{F\left( b \right) - F\left( a \right)}}{{b - a}}$$

等式左边代表的是这段区间上函数的平均高度（或者说这段区间上导数的平均值），等式的右边是原函数的从区间起点到区间终点的割线斜率，微积分基本定理实际上可以描述为：一段区间上导数的平均值等于原函数区间起点和终点的割线卸率

> 多元积分里的格林公式，高斯公式固然也非常神奇，散度和旋度的概念虽然也很重要，但是这些东西具有极强的物理背景，正常应用中按理说不会涉及，这里不做研究

### 2.4 卷积

> 卷积的定义在多处见过，如`CNN`的核心卷积层，信号分析的冲激响应使用卷积计算，那么卷积的这个卷到底是如何体现的呢，什么情况下才使用卷积

> 理解卷积没什么用，这里纯纯是为了解答当时的一些疑惑吧

* 直观理解

$$\int_a^b {f\left( x \right)g\left( {z - x} \right)} dx$$

其中 $z = x + y$ 

上述的积分可以理解为在区间 $[a, b]$ 上求直线的 $z = x + y$ 的线密度，当 $z$ 变化时，直线会滑过平面，就像我们卷膜布时一样，因此我们将这种积分方式称为卷积

* 信号分析和`CNN`中的卷积

$$(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau) d\tau$$

$g(t-\tau)$ 为到 $t$ 时刻时的冲激响应

$$(I * K)(i, j) = \sum_{m} \sum_{n} I(m, n) K(i-m, j-n)$$

其中 $K$ 为卷积核， $I$ 为输入的图像大小

实际上卷积没那么讳莫如深，卷这个字固然形象但没有描述出卷积的实质，不算个好的名字

卷积本质上是对一整条输入做一个滑动窗口加和的操作，区别在于`CNN`中的卷积核是训练出来的