# 读研以来对高等数学新的认识

> 本科期间的**微积分，线性代数，概率论与数理统计**可谓是最重要的三门数学课，是所有研究应该掌握的基础。但是很遗憾，得益于本科教育垃圾的教学模式，应试的考察方式和狗屎一样的教材，我这几门课和没学区别也不是很大。

> 读研以来，对几门数学的理解驻日加深，虽然不知道理解了有什么卵用，但是系统地学会一些知识总是好的，特在此整理了笔记，梳理一下我的一些关键理解

> * 首先感谢华科矩阵论的教师**崔洪勇老师**，他教授的矩阵论课程深入浅出，让我真正开始弄懂了矩阵的本质
> * 然后我还要感谢伟大的**Gemini**，和**Gemini**交谈的过程中让我顿悟了许多东西
> * 最后要感谢`bilibili`的`up`主`3Blue1Brown`和知乎优质的创作者们，你们的分享给了我大量的帮助


## 1 矩阵论

### 1.1 **矩阵是线性变换**

一切的一切首先要从**矩阵是线性变换**这个结论开始，线性空间的性质和定义这里不做赘述。

> 虽然不做赘述，但线性空间的定义是整个矩阵论和根基，就如同极限的定义是整个微积分的根基一样，只有定义了线性空间，我们的整套理论才有讨论的价值，要时刻铭记这一点

线性空间的概念是十分宽泛的，实际上我们平常处理的最多的是向量空间，除此以为还有矩阵空间，多项式空间和函数空间，而维度为2的向量空间是最经典的欧式空间。后面的讨论也以最好可视化的向量空间为例

![线性空间](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性空间.png)

向量组定义在向量空间里，是向量的聚合方式（比如神经网络中的一个`batch`构成一个向量组），而矩阵本质上是线性变换的一个算子，矩阵通过和向量相乘将向量映射成另一个向量

> 这里我还是想吐槽一下编写线性代数课本的这些傻卵，把如此成体系的知识讲的毫无逻辑可言。上来就讲行列式，课程推进的过程中矩阵，向量组，行列式交杂在一起，让我一度以为这几个东西就是一回事，一通学下来只会做几道破题，舍本逐末

以下是一个线性变换的例子，可以将以标准正交基为底的向量空间中的向量旋转 $\theta$ 角度

![旋转变换](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/旋转变换.png)

更为普遍地，由空间的基底在该线性变换下的映射可轻松得出该线性变换的矩阵：

![线性变换和矩阵](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性变换和矩阵.png)

进一步对于任意向量，通过矩阵与其坐标的相乘可以快速得到线性变换后的坐标：

![线性变换和矩阵2](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/线性变换和矩阵2.png)

> **矩阵是线性变换的容器，而向量组是某个线性空间下储存数据的容器**，之前的学习过程中不应该将两者混为一谈
> 秩是描述向量组中最大线性无关组个数的定义，维度是描述线性空间本身的定义，更不应该拿这两种定义来描述矩阵

### 1.2 特征值和特征向量

容易想到，矩阵作为线性变换实际上需要以基底为基础求出，当基底不同时，同一个线性变换的对应的矩阵可能不同，以下是一个例子：

设定二维空间 $\mathbb{R}^2$ 上的变换规则：

$$T\left(\begin{bmatrix} x \\ y \end{bmatrix}\right) = \begin{bmatrix} 2x \\ x+y \end{bmatrix}$$

选取标准基 $\mathcal{E} = \{e_1, e_2\}$，其中 $e_1=\begin{bmatrix} 1 \\ 0 \end{bmatrix}, e_2=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$。

$$T(e_1) = \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \mathbf{2}e_1 + \mathbf{1}e_2$$

$$T(e_2) = \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \mathbf{0}e_1 + \mathbf{1}e_2$$

提取系数，得到矩阵 **$A$**：

$$A = \begin{bmatrix} 2 & 0 \\ 1 & 1 \end{bmatrix}$$

选取新基底 $\mathcal{B} = \{v_1, v_2\}$ ，其中 $v_1=\begin{bmatrix} 1 \\ 1 \end{bmatrix}, v_2=\begin{bmatrix} 1 \\ 0 \end{bmatrix}$

$$T(v_1) = \begin{bmatrix} 2 \\ 2 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \mathbf{2}v_1 + \mathbf{0}v_2$$

$$T(v_2) = \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 1\begin{bmatrix} 1 \\ 1 \end{bmatrix} + 1\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \mathbf{1}v_1 + \mathbf{1}v_2$$

提取系数，得到矩阵 **$B$**：

$$B = \begin{bmatrix} 2 & 1 \\ 0 & 1 \end{bmatrix}$$

事实上，不同基底下的矩阵是相似的（从这个角度理解想似对角化比较好）：

![矩阵相似](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/矩阵相似.png)

那么其实如果只给定你一个矩阵，不告知你基底是什么，实际上我们无法直接知道关于这个线性变换的更多信息，这时候我们需要发掘矩阵的核心特质了，同一个线性变换对应的不同矩阵，有没有什么共同点呢？

$$AX = \lambda X$$

以上是特征值和特征向量的定义式，在线性代数初次见到它的时候，我对什么是特征值和什么是特征向量毫无概念，如今，我终于能深刻理解这两个概念的意义了

**特征向量是变换中的方向不变的向量，特征值是被拉伸或压缩的倍数。在线性变换过程中，绝大多数向量的方向都会改变，总有那么几个特殊的向量，在变换前后，它们的方向完全没有改变，依然保持在原来的直线上。这些特殊的向量就是特征向量**

特征向量是线性变换的主轴：  

![主轴](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/主轴.png)

**同一个线性变换对应的不同矩阵，特征值完全相等，特征向量的几何实体相等**（坐标可能不等，但不重要）

### 1.3 行列式

在充分理解了矩阵的本质后，我们终于可以描述行列式的意义了

> 这里第三次吐槽傻卵线性代数，上来就教行列式，这能学的懂就有鬼了，还没学会走路就开始跑了，就因为行列式好计算，好出题，他妈的就是纯为了做题而生的

可以想到，经过线性变换后，整个向量空间被映射到了另一个向量空间上，如何衡量整个空间的放缩尺度，这时候行列式应运而生。行列式有着极强的几何意义，并非只是让你算着玩玩和判定一些条件的工具

行列式 $\det(A)$ 是线性变换对空间**有向体积**的伸缩倍数：

考虑在二维平面上，我们有一个标准的单位正方形，它的边由基底向量 $\vec{i}=(1,0)$ 和 $\vec{j}=(0,1)$ 围成，面积显然是 $1 \times 1 = 1$

当我们用一个矩阵 $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ 作用于这个平面时：基底 $\vec{i}$ 变成了向量 $(a, c)$ ，基底 $\vec{j}$ 变成了向量 $(b, d)$ 

原来的单位正方形，被拉扯变成了一个平行四边形。行列式的值 $\det(A) = ad - bc$，正是这个变换后的平行四边形的有向面积。

* 行列式为0

**行列式为0说明经过线性变换后的空间损失了维度，故面积为0，这也意味着变换过程损失了信息，因此行列式为0是矩阵不可逆的判定条件**

* 行列式的符号

**$\det(A) > 0$（正）：** **保持手性**。空间的相对方向不变（右手系依然是右手系）
**$\det(A) < 0$（负）：** **翻转手性**。空间发生了的翻转（右手系变为左手系）

* 雅可比行列式

> 在微积分的二重积分换元时，我第一次看到了雅可比行列式的概念，很遗憾，课本上直接抛出了这个概念，并未给出任何解释，我当时只是隐约知道这是一个缩放因子

$$J = \begin{pmatrix} 
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ 
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} 
\end{pmatrix} $$

雅可比矩阵描述了换元的对应关系，实质上是一个映射，很显然，这个矩阵的行列式的值描述了换元后微元的面积变化情况，所以在进行换元时要乘这个面积缩放因子

### 1.4 奇异值分解

上面的讨论主要基于方阵，当矩阵的行列不相等时，虽然也表示线性变换，但这个时候特征值的定义方程是 $Ax = \lambda x$。如果 $A$ 是 $m \times n$ 的矩阵（ $m \neq n$），会导致输入的向量和输出向量维度不同 

**这就导致 $Ax$ 和 $x$ 属于不同的空间，无法进行比较（无法说 $Ax$ 是 $x$ 的 $\lambda$ 倍）**

这时候为了求特征值，自然可以想到：

$${A^H}A{v_i} = {\lambda _i}{v_i}$$

求解来说参照以下过程：

![SVD求解](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/SVD求解.png)

分析求解过程中我们可以看到：

$$A{v_i} = \sigma {u_i}$$

**实际上奇异值和特征值描述的是同一套东西，不同的是需要给出特征向量在两个不同维度空间里的表达**

进一步来说，对于任意矩阵 $A_{m \times n}$，我们可以将其分解为三个矩阵的乘积：

$$A = U \Sigma V^T$$

这里的三个部分分别对应了特征值和特征向量的推广概念：

1. $\Sigma$ (奇异值矩阵)： 这是一个 $m \times n$ 的对角矩阵。它们代表了变换在不同方向上的“拉伸”程度

2. $V$ (右奇异向量矩阵)： 这是一个 $n \times n$ 的正交矩阵。它们类似于输入空间中的特征向量方向

3. $U$ (左奇异向量矩阵)： 这是一个 $m \times m$ 的正交矩阵。它们是输出空间中对应的基向量

$$A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \dots + \sigma_r u_r v_r^T$$

* `PCA`分析

这里顺带提一下之前机器学习中使用的主成分分析，这本质就是一个`SVD`过程

> 写到这里的时候累了，不想继续深挖探究了，以后再说吧

### 1.5 秩-零度定理（维度守恒定理）

这里先介绍秩-零度定理，对于一个 $m \times n$ 的矩阵 $A$（或者一个从 $n$ 维空间到 $m$ 维空间的线性变换 $T$）有：

$$\text{Rank}(A) + \text{Nullity}(A) = n$$

其中，`n` 即输入的自由度，秩`Rank(A)`为矩阵列空间的维度，零度`Nullity(A)`为矩阵零空间的维度

#### 1.5.1 列空间（像空间）

首先我们必须明确**矩阵的列空间的维度直接决定输出的维度**，假设 $A$ 的列向量为 $\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_n$，向量 $\mathbf{x}$ 的元素为 $x_1, x_2, \dots, x_n$。那么：

$$A\mathbf{x} = x_1\mathbf{c}_1 + x_2\mathbf{c}_2 + \dots + x_n\mathbf{c}_n$$

可以清楚地看到，输出的结果实际是列向量的线性组合，**我们如果改变 $\mathbf{x}$ 的值，输出的所有向量实际上构成了一个像空间，很显然，像空间的维度等于列空间维度（同一套基底），所以我们又把列空间叫像空间**

#### 1.5.2 零空间

从零空间解释维度守恒定理时我查阅了很多资料，始终无法理解，列空间实际上对应了输出空间的维度，而零空间是映射成0的解空间的维度，两者为什么可以相加，且相加结果等于`n`

根据以往的经验，硬去理解一个解释效果是非常不好的，无法理解时常常是因为相关的知识储备不足，这里放弃解释维度守恒定理为什么会成立，等待以后某个时机去解释吧

#### 1.5.3 简单应用

* 线性方程组解的判定条件
1. **齐次线性方程组 $Ax=0$**  
    - 若 $\mathrm{rank}(A) = n$（ $n$ 为未知数个数），只有零解。
    - 若 $\mathrm{rank}(A) < n$ ，有无穷多非零解。

2. **非齐次线性方程组 $Ax=b$**   
    - 若 $\mathrm{rank}(A) = \mathrm{rank}([A|b]) = n$ ，有唯一解。
    - 若 $\mathrm{rank}(A) = \mathrm{rank}([A|b]) < n$ ，有无穷多解。
    - 若 $\mathrm{rank}(A) \neq \mathrm{rank}([A|b])$ ，无解。

依据`维度守恒定理`和`像空间维度等于列空间维度`即可轻松解释整组判定条件

* 矩阵的行秩等于列秩

> 这个结论实际上非常不直观不显然，当时学的时候也是直接抛出了这个结论（其实是给了实例，矩阵经过行列初等变化后发现相等，在我看来非常不够让人信服），我查阅了很多资料都不能理解

首先，**矩阵的行空间和零空间是正交互补的，因为零空间的描述的是与行空间必定点积等于0的向量构成的空间**，所以有：
$$\dim(\text{行空间}) + \dim(\text{零空间}) = n$$

与维度守恒定律联立即可获得矩阵的行秩等于列秩的结论

> 这个解释我体感上仍然不够涉及本质，因为秩-零度定理也没有得到一个很好的理解，这一部分暂且这样吧

### 1.6 叉积和对偶原理

> 前面的部分全部建立在矩阵是线性变换的基础上，都是对线性空间中线性变换的讨论，这一部分，我们讨论线性空间本身的一些性质和内容

* 内积

* 叉积

* 对偶原理

## 2 微积分

### 2.0 极限和无穷小

* 函数极限

极限的定义是整个微积分的基础，这里重新给出极限的 $\varepsilon-\delta$ 的描述：

设函数 $f(x)$ 在点 $x_0$ 的某个去心邻域内有定义。如果对于任意给定的正数 $\varepsilon > 0$ ，总存在一个正数 $\delta > 0$，使得当自变量 $x$ 满足 $0 < |x - x_0| < \delta$ 时，对应的函数值 $f(x)$ 都满足：$$|f(x) - A| < \varepsilon$$那么常数 $A$ 就叫做函数 $f(x)$ 当 $x \to x_0$ 时的极限，记作：$$\lim_{x \to x_0} f(x) = A$$

* 无穷小

很显然，**极限描述了一个目标（或者说逼近的结果）**，但是相同极限的不同函数在逼近的过程种性质似乎是不同的，这就需要定义无穷小量去描述这个过程：

如果函数 $\alpha(x)$ 当 $x \to x_0$ 时的极限为零，即 $\lim_{x \to x_0} \alpha(x) = 0$，那么称函数 $\alpha(x)$ 为当 $x \to x_0$ 时的无穷小量

需要特别注意的是，**无穷小并非一个极小的静态数，而是一个极限为零的动态变量，因此我们可以定义出高低阶无穷小（大），无穷小实际上描述了函数向极限的收敛速度（无穷大描述了发散速度）**

借助无穷小的概念，我们可以将极限的定义重新写为一个非常直观的代数关系：$\lim_{x \to x_0} f(x) = A$ 的充要条件是 

$$f(x) = A + \alpha(x)$$

我认为有了无穷小的定义，极限的定义才算完美，**极限本身描述了结果，而无穷小描述了过程（与函数本身相关）**

> 这个无穷小实际上可以使用泰勒级数（研究函数在某一点附近的性质最好用的工具）进行量化分析

> 极限不同于我们理解上的一般意义的数，但幸运地是，借助无穷小量的定义及其配套的理论体系，我们可以分析极限的运算法则，并发现极限实际上满足我们熟知的数的运算法则，这里不做赘述

### 2.1 `e`的本质

`e`的课本上的定义式如下：

$$e = \mathop {\lim }\limits_{n \to \infty } {\left( {1 + \frac{1}{n}} \right)^n}$$

或者可以用无穷级数定义：

$$e = \sum_{n=0}^{\infty} \frac{1}{n!} = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots$$

但这为什么那么多公式里面都有`e`，上面的定义不能给出直观的理由

在理解了特征值和特征向量后，这里有更涉及本质的解释：

首先，要把函数定义成线性空间，对于相同定义域上的连续实函数有：

$$(f+g)(x) = f(x) + g(x)$$

$$(c \cdot f)(x) = c \cdot f(x)$$

可以认为在这一个定义域上的所有函数构成一个线性空间。这里卖一个关子，这个线性空间的基底怎么写，整体函数空间也太大了（靠傅里叶变换）

定义了函数的线性空间后，实际上如果把求导看成一种算子 $D$ ，那么求导实际上是一个线性变换：

$$D\left( {f + k \cdot g} \right) = Df + k \cdot Dg$$

下面的例子展示了多项式空间中求导对应的矩阵：

![求导的矩阵](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/求导的矩阵.png)

接下来就好说了，直接给出下面的两个公式：

$$AX = \lambda X$$

$$D\left( {\lambda {e^{\lambda x}}} \right) = \lambda {e^{\lambda x}}$$

这时候我震惊的发现：**$e^{\lambda x}$ 是微分算子的特征向量**

这是`e`的本质是变化率的最直观的解释

> 很可惜，以上的两个公式是和`Gemini`闲聊时获得的，并非我自己想到的，但在看到这两个公式的一瞬间，我就瞬间意识到了它们意味着什么，大脑甚至宕机了一段时间

至于`e`是如何定义的，考虑对指数函数求导：

$$\frac{d}{{dx}}{a^x} = \mathop {\lim }\limits_{\Delta x \to 0} \frac{{{a^x}{a^{\Delta x}} - {a^x}}}{{\Delta x}} = \mathop {\lim }\limits_{\Delta x \to 0} {a^x}\frac{{{a^{\Delta x}} - 1}}{{\Delta x}}$$

如果希望 $\mathop {\lim }\limits_{\Delta x \to 0} \frac{{{a^{\Delta x}} - 1}}{{\Delta x}} = 1$

那么很显然：

$$a = \mathop {\lim }\limits_{\Delta x \to 0} {\left( {1 + \Delta x} \right)^{\frac{1}{{\Delta x}}}}$$

我们可以把这里的底数`a`定义为`e`

至于上述极限怎么求解，我认为应该有了`e`的性质以后，通过泰勒级数求解，而不是直接通过上述极限获得`e`的值

### 2.2 微分的一些认识

从求导开始，这里还是要重写一下求导的定义：

$$\mathop {\lim }\limits_{\Delta x \to 0} \frac{{f\left( {x + \Delta x} \right) - f\left( x \right)}}{{\Delta x}}$$

求导是求极限的过程，要时刻铭记这一点

* 连续不一定可导，可导一定连续

这个还是比较好理解的，回忆一下函数连续的定义：

$$\mathop {\lim }\limits_{\Delta x \to 0} f\left( {x + \Delta x} \right) = f\left( x \right)$$

容易看出导数存在是比函数连续的更强的条件

* 微分和函数变化量

![图解微分](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2026/图解微分.png)

$$\Delta y = 2x \cdot dx + {\left( {dx} \right)^2}$$

$$dy = 2x \cdot dx$$

从上图看出微分并不是函数变化的全部，$dy$ 只保留了和 $dx$ 变换中完全线性的部分而忽略了 $dx^2$ ，而保留的部分实际上就是导数作用的部分

这是微分处理的核心所在，**认为函数在极小的领域上关于变化量是线性变化的**

* 一阶微分的形式不变性

这个结论其实没什么用且很好理解，在这里重新回顾它是因为我当初在学习的过程中对这个结论糊里糊涂的，今天直接把它给分析透了

这个结论描述的其实是复合函数的微分法则，可以直接对中间变量进行微分：

$$y = f\left( u \right),u = g\left( x \right)$$

$$dy = f'\left( u \right)du = f'\left( u \right)g'\left( x \right)dx$$

而情况到了二阶微分以后：

$${d^2}y = d\left( {f'\left( u \right)du} \right) = f''\left( u \right)d{u^2} + f'\left( u \right){d^2}u$$

这里由于 ${d^2}u \ne 0$ （实际上 ${d^2}x = 0$ ），所以 ${d^2}y \ne f''\left( u \right)d{u^2}$

> 课本上能把简单的事情说这么复杂也是真神人了，典型的不说人话

* 泰勒展开

可以想到，微分希望的是在函数某个点极小的领域内使用求 $dx$ 的线性近似，如果我们希望可以更进一步精确一些，或者我们希望这个领域就算大一点我们也可以用类似的多项式去近似函数，泰勒展开就是为此而生的

无穷级数形式：

$$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n$$

泰勒中值定理形式：

$$f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)$$

### 2.3 多元微分和梯度

* 全微分

和一元微分是同一种思路，用关于微元的线性变化去表示多元函数的变化量

$$dz = \frac{{\delta z}}{{\delta x}}dx + \frac{{\delta z}}{{\delta y}}dy$$

全微分存在的充分条件是该点的所有偏导数全部连续

> 这个条件更强，一元微分仅仅要求该点的导函数存在，一元函数因为方向太少，导数存在与可微是等价的。多元函数因为方向太多，各个方向偏导数都存在不足以保证全微分存在，必须加上偏导数连续这个充分条件，才能压制住其他方向的异常突变

* 方向导数和梯度

比较容易可以想象到，只研究偏导数是不够的，偏导数只给出了几个特定方向上的性质，如果我们希望研究关于任意方向上的形式，这时候就需要方向导数的存在：

$${\left. {\frac{{\delta f}}{{\delta l}}} \right|_{\left( {{x_0},{y_0}} \right)}} = {f_x}\left( {{x_0},{y_0}} \right)\cos \alpha  + {f_y}\left( {{x_0},{y_0}} \right)\cos \beta $$

以当前点上的两个偏导数作为基底，可以组合出任意方向上的导数

对于一元函数，自变量只有两个方向可以走，但是到了多元函数中方向是有无穷多个的，我们自然想找出一个方向，函数沿着这个方向走时，变化最为大

$${\left. {\frac{{\delta f}}{{\delta l}}} \right|_{\left( {{x_0},{y_0}} \right)}} = \nabla f\left( {{x_0},{y_0}} \right) \cdot {e_l}$$

很显然，观察方向导数的定义式，最大的方向导数模长为 $\sqrt {{f_x}{{\left( {{x_0},{y_0}} \right)}^2} + {f_y}{{\left( {{x_0},{y_0}} \right)}^2}} $ ，此时方向与梯度方向相同

> 之前的认识里将梯度和导数混为一谈是不对的，**梯度指出了一个方向，而导数/微分分析的是具体变化量；梯度是多元分析里最重要的工具**，它的概念是如此重要，神经网络的优化算法完全依赖于梯度下降法

* 曲面法向量

空间曲面 $F(x,y,z) = 0$ 的法向量就是梯度,这很好理解,当我们将曲面定义为 $F(x, y, z) = 0$ 时，意味着在这张曲面上的每一个点，函数 $F$ 的值都是恒定的

而梯度指向函数值变化最快的方向,要想让函数值变化最快，必须沿着最决绝地离开当前等值面的方向移动，也就是垂直于该曲面的方向

数学证明:

假设曲面 $F(x, y, z) = 0$ 上有一条任意曲线 $\mathbf{r}(t) = (x(t), y(t), z(t))$

因为曲线在曲面上，所以：

$$F(x(t), y(t), z(t)) = 0$$

对时间 $t$ 两边求导：

$$\frac{\partial F}{\partial x}\frac{dx}{dt} + \frac{\partial F}{\partial y}\frac{dy}{dt} + \frac{\partial F}{\partial z}\frac{dz}{dt} = 0$$

这正好可以写成梯度向量和曲线切向量的点积：$$\nabla F \cdot \mathbf{r}'(t) = 0$$

由于 $\mathbf{r}'(t)$ 是曲面上的切向量，且点积为 0，这证明了 $\nabla F$ 垂直于曲面上任意的切向量，因此它是法向量。

### 2.4 积分的一些认识

* 面积的定义
  
在开始之前首先要明确，积分来源于求解函数图像下围成的东西的面积，最终却定义了这些面积是什么

首先是一条公理：一个长为 $w$、宽为 $h$ 的矩形，其面积定义为 $A = w \times h$ 这是积分计算面积的前提

接下来黎曼对积分进行了定义，具体的说，在区间 $[a, b]$ 上具有有限个间断点的函数可以积分（充分条件，这里提一嘴迪利克雷函数，处处不连续，处处不可导，这样的函数就不能用黎曼积分做）

黎曼积分定义为：

$$\int_a^b {f\left( x \right)dx}  = \mathop {\lim }\limits_{\lambda  \to 0} \sum\limits_{i = 1}^n {f\left( {{\xi _i}} \right)\Delta {x_i}} $$

其中 $\lambda  = \max \left( {\Delta {x_i}} \right)$ ，是的，积分也是一个极限

* 微积分基本定理

$$\int_a^b {f\left( x \right)dx}  = F\left( b \right) - F\left( a \right)$$

很显然 ${f\left( x \right)dx}$ 是原函数的微分，对微分求积分（求和），就可以得到原函数在一个区间上的变化量

> 实际上，**微积分基本定理描述了一个求积分面积——求微分叠加——求原函数变化量的转化过程**，这是非常伟大的，统一了微分和积分过程

> 当初在学习时，直接机械地认为积分是微分/求导的逆运算是不对的认知，这样体会不到微积分基本定理的意义。积分首先是根据求面积定义的，借助微积分基本定理，我们可以使用求微分的逆运算的过程解决这个积分问题

相较于一般形式，`3Blue1Brown`给出的形式也比较便于理解：

$$\frac{{\int_a^b {f\left( x \right)dx} }}{{b - a}} = \frac{{F\left( b \right) - F\left( a \right)}}{{b - a}}$$

等式左边代表的是这段区间上函数的平均高度（或者说这段区间上导数的平均值），等式的右边是原函数的从区间起点到区间终点的割线斜率，微积分基本定理实际上可以描述为：一段区间上导数的平均值等于原函数区间起点和终点的割线卸率

> **多重积分虽然定义了 $d\sigma$（面积微元）或 $dV$（体积微元），但在操作层面上，没有与之相关的微分体系，没有发明出二重不定积分公式或三重不定积分公式，实际上是转化成了多个一元积分来处理，所以没有多元积分只有多重积分**，涉及到相关的只有计算技巧，因此不做过多讨论

> 一二型曲线曲面积分具有极强的物理背景，格林公式和高斯公式虽然令人印象深刻，散度和旋度的概念虽然具有很深刻的意义，**但这些实际上讨论的是向量值函数如何使用非向量方法分析，它们把复杂的向量场问题，转化为了简单的标量积分问题**，正常应用中感觉很难涉及，这里也不讨论

### 2.5 傅里叶变换

前面提到了，我们可以讲定义在某个区间上的函数定义为一个线性空间，可是，不同于向量空间，找出函数空间的一组基底似乎是非常困难的

* 泰勒级数

泰勒级数给出了一种方式，用一组多项式作为函数空间的基底

泰勒级数选取的基底是单项式：

$$\{1, x, x^2, x^3, \dots\}$$
 
泰勒级数是局部的。它极度依赖于某一点的导数信息。离这一点越远，逼近效果越差

在引出傅里叶变换之前，需要先定义好函数空间的内积：

$$\langle f,g\rangle  = \int_b^a f (x)g(x)dx$$

> * 函数空间的内积定义
> 如果我们将区间 $[a, b]$ 分割成 $n$ 个点，那么函数 $f(x)$ 在这些点上的值就可以看作一个 $n$ 维向量：
> $$\vec{v}_f = [f(x_1), f(x_2), \dots, f(x_n)]$$
> 两个 $n$ 维向量 $\vec{v}_f$ 和 $\vec{v}_g$ 的点积定义为对应分量的乘积之和：
> $$\vec{v}_f \cdot \vec{v}_g = \sum_{i=1}^n f(x_i) g(x_i)$$
> 那自然可以想到：
> $$\lim_{n \to \infty} \sum_{i=1}^n f(x_i) g(x_i) \Delta x = \int_a^b f(x)g(x) dx$$
> * 几何意义
> 在这种定义下，我们可以定义两个函数之间的距离：
> $$\|f\| = \sqrt{\langle f, f \rangle} = \sqrt{\int_a^b |f(x)|^2 dx}$$
>  同样可以定义两个函数垂直，如果两个向量垂直，它们的点积为0，则有：
> $$\int_a^b f(x)g(x) dx = 0$$

很显然，在这个定义下，泰勒级数的基底并不标准正交，或者说，泰勒级数倾向于近似一个数值，而不是将任意一个函数分解为基底之和

* 傅里叶级数的三角形式

天才的傅里叶级数采取了另一种思路，它使用基底：

$$\{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots\}$$

> 这一分解一开始来源于周期函数可以分解为三角函数叠加的直觉

在这组基底里，任何两个不同的波，它们的内积永远是 0

$$\int_{-\pi}^{\pi} \sin(nx) \cos(mx) dx = 0$$

$$\int_{-\pi}^{\pi} \sin(nx) \sin(mx) dx = 0 \quad (n \neq m)$$

傅里叶级数可以表达为（关于基底的坐标式）：

$$f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} \left( a_n \cos nx + b_n \sin nx \right)$$

其中， 

$$a_0 = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \, dx$$

$$a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos nx \, dx \quad (n=1, 2, 3, \dots)$$

$$b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin nx \, dx \quad (n=1, 2, 3, \dots)$$

几个系数的求法实际上是比较好理解的，这里的坐标不像向量空间一样可以轻松观察得出，但是我们定义了函数空间的内积，因此可以通过正交投影获得这几个坐标：

$$c_n = \frac{\vec{v} \cdot \vec{e_n}}{\vec{e_n} \cdot \vec{e_n}} = \frac{\text{投影}}{\text{基向量模长的平方}}$$

具体来说：

$$a_n = \frac{\int_{-\pi}^{\pi} f(x) \cos nx \, dx}{\pi} = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos nx \, dx$$

* 傅里叶级数的指数形式

事实上，由于欧拉公式，我们还可以得到傅里叶变换的指数形式

$$f(x) = \sum_{n=-\infty}^{\infty} c_n e^{inx}$$

$$c_n = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) e^{-inx} \, dx$$

指数形式使用基底： $\{\dots, e^{-i2x}, e^{-ix}, 1, e^{ix}, e^{i2x}, \dots\}$

需要注意的是由于引入了复数，此时的内积定义为：

$$\langle f, g \rangle = \int_{-\pi}^{\pi} f(x) \overline{g(x)} \, dx$$

> 指数形式的傅里叶变化更强
> * 三角形式描述的是**振荡**（实轴上的往复运动）
> * 指数形式 $e^{inx}$ 描述的是**旋转**（复平面上的圆周运动）
> * 任何一个周期信号，都可以看作是无数个以不同速度（频率 $n$）和不同半径（幅度 $|c_n|$）旋转的圆矢量叠加而成的

* 从傅里叶变换到拉氏变换

我们可以把傅里叶变换的定义域推广开到 $\left[ { - \infty , + \infty } \right]$ ，此时，**基底变为 ${e^{iwt}}$ ，分量不再是正整数幂**，所以这里必须使用积分而不是求和来进行分解：

$$f(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} F(\omega) e^{i\omega t} \, d\omega$$

$$F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i\omega t} \, dt$$

> $F(\omega)$ 函数实际上描述了一个时域函数在频域上的强度

但是这时候有一个很强的条件限制：信号必须绝对可积

$$\int_{-\infty}^{\infty} |f(t)| \, dt < \infty$$

如果不满足，显然傅里叶变换的系数是积不出来的

在现实世界（特别是控制系统）中，我们经常要研究不稳定系统。它们的信号可能是 $f(t) = e^{2t}$。这是一个指数爆炸增长的信号。

拉氏变换的策略是用衰减得更快的因子 $e^{-\sigma t}$ 去压制它，使得变换的系数可积，是一种工程学考量：

$$\int_{0}^{\infty} f(t) e^{-\sigma t} e^{-i\omega t} \, dt = \int_{0}^{\infty} f(t) e^{-(\sigma + i\omega)t} \, dt$$

完整的拉式变换如下所示：

$$L(s) = \int_{0}^{\infty} f(t) e^{-st} \, dt$$

$$f(t) = \frac{1}{2\pi i} \int_{\sigma - i\infty}^{\sigma + i\infty} L(s) e^{st} \, ds$$

**从几何上看，拉氏变换实际上是把傅里叶变换从一维推广到了二维。傅里叶变换只在复平面的虚轴上选取基底，拉氏变换则在这个复平面上全域探索**

> 至此，从函数空间出发推导到拉式变换，和控制理论也串联起来了

## 3 其他

> 其他东西不好整理成一个章节，这里就零零散散记录一些吧

### 3.1 卷积

> 卷积的定义在多处见过，如`CNN`的核心卷积层，信号分析的冲激响应使用卷积计算，那么卷积的这个卷到底是如何体现的呢，什么情况下才使用卷积

> 理解卷积没什么用，这里纯纯是为了解答当时的一些疑惑吧

* 直观理解

$$\int_a^b {f\left( x \right)g\left( {z - x} \right)} dx$$

其中 $z = x + y$ 

上述的积分可以理解为在区间 $[a, b]$ 上求直线的 $z = x + y$ 的线密度，当 $z$ 变化时，直线会滑过平面，就像我们卷膜布时一样，因此我们将这种积分方式称为卷积

* 信号分析和`CNN`中的卷积

$$(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau) d\tau$$

$g(t-\tau)$ 为到 $t$ 时刻时的冲激响应

$$(I * K)(i, j) = \sum_{m} \sum_{n} I(m, n) K(i-m, j-n)$$

其中 $K$ 为卷积核， $I$ 为输入的图像大小

实际上卷积没那么讳莫如深，卷这个字固然形象但没有描述出卷积的实质，不算个好的名字

卷积本质上是对一整条输入做一个滑动窗口加和的操作，区别在于`CNN`中的卷积核是训练出来的

### 3.2 中心极限定理和正态分布

### 3.3 贝叶斯公式

> * 写在最后
> 