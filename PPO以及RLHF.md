# PPO和RLHF

## 1 `PPO`回顾

在此之前的`DRL`的学习过程中，我虽然实现了`PPO`方法，但是对其一些本质仍然称不上理解，借学习`RLHF`的机会，重新对其几个重要概念进行了理解

* 优势和损失函数

首先是优势的定义：

$${A_t} = {Q_\pi }\left( {{s_t},{a_t}} \right) - {V_\pi }\left( {{s_t}} \right)$$

在`PPO`中，我们借助贝尔曼方程和蒙特卡洛估计方法：

$${Q_\pi }\left( {{s_t},{a_t}} \right) = {E_{{s_{t + 1}}}}\left[ {{r_t} + \gamma {V_\pi }\left( {{s_{t + 1}}} \right)} \right]$$

可以将优势写为：

$${\hat A_t} = {r_t} + \gamma {V_\pi }\left( {{s_{t + 1}}} \right) - {V_\pi }\left( {{s_t}} \right)$$

如果 ${\hat A_t}$ 大于0，说明，动作 ${a_t}$ 比平均水平好，反之亦然

所以在`PPO`的前身`A2C`中。我们将策略网络的损失函数写为：

$${L^{A2C}}\left( \theta  \right) = \ln \pi \left( {{a_t}|{s_t};\theta } \right){\hat A_t}$$

注意 ${\pi \left( {{a_t}|{s_t};\theta } \right)}$ 是选定的动作对应的概率,而不是概率分布，取对数的意义是希望小概率的动作在计算梯度时有更大的优势

`PPO`在此基础上引入了改进：

$$L^{\text{CLIP}}(\theta) = \min \left( r_t(\theta) \hat{A}_t, \quad \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right)$$

其中的 ${{r_t}\left( \theta  \right)}$ 为：

$${r_t}\left( \theta  \right) = \frac{{\pi \left( {{a_t}|{s_t};\theta } \right)}}{{\pi \left( {{a_t}|{s_t};{\theta _{old}}} \right)}}$$

这个比率和梯度的截断是`PPO`可以进行多更新的关键， ${\pi \left( {{a_t}|{s_t};{\theta _{old}}} \right)}$ 是网络在采样时选定动作的概率, 而 ${\pi \left( {{a_t}|{s_t};\theta } \right)}$ 是网络在训练时选定动作的概率

比率充当“汇率转换器”，修正新旧策略的分布差异，使得旧数据可以合法地用于更新新策略（复用数据）。截断 (Clip) 充当“安全熔断器”，当比率变化过大时，强制停止奖励，限制更新幅度。

下面是一个具体的例子说明为什么可以多轮更新：

> 第一阶段：采样
> 在此阶段，我们使用旧策略 ($\pi_{old}$) 与环境交互，收集数据。
> * 旧策略概率: $\pi_{old}(a|s) = 0.4$
> * 生成数据: 锁定数据样本 $\{ s, a, \pi_{old}=0.4, \hat{A}=+2.0 \}$。
> * 状态: 此时数据已固定，$\pi_{old}$ 作为一个常数不再改变。

> 第二阶段：训练循环
> 利用同一批数据，对新策略进行多次梯度更新。
> 
> 1: 初始更新
> * 当前状态: $\pi_{new}$ 刚初始化，等于 $0.4$。
> * 计算比率: $r_t = \frac{\pi_{new}}{\pi_{old}} = \frac{0.4}{0.4} = 1.0$。
> * 结果: 正常更新，优化器将 $\pi_{new}$ 提升至 0.5。
> 
> 2: 修正偏差 
> * 当前状态: $\pi_{new}$ 已变为 0.5
> * 计算比率: $r_t = \frac{0.5}{0.4} = \mathbf{1.25}$。
> * 机制：比率 $1.25$ 作为修正权重 
> * 结果: 有效更新，优化器继续将 $\pi_{new}$ 提升至 0.6。
> 
> 3: 触发截断 
> * 当前状态: $\pi_{new}$ 已变为 0.6。
> * 计算比率: $r_t = \frac{0.6}{0.4} = \mathbf{1.5}$。
> * 机制：
>     * 比率 $1.5$ 超过了允许范围 $[1-\epsilon, 1+\epsilon] = [0.8, 1.2]$。
>     * 触发 Clip: 梯度计算中的比率被强制锁定为 1.2。
> * 结果:
>     * 算法忽略多出的部分 ($1.5 \to 1.2$)。
>     * 停止激进更新，防止新策略偏离旧策略太远导致崩塌。

![PPO梯度截断](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2025/PPO梯度截断.png)

* `PPO`用于连续动作空间

我的`github`仓库中给出了`PPO`用于连续动作空间的完代码,用于连续动作空间时的核心公式并无不同，此时策略网络并不直接输出动作的概率分布，这里我们假设动作服从高斯分布，策略网络输出的是这个分布的参数：均值 $\mu$ 动作的中心值。标准差 $\sigma$ 动作的离散程度。

> 关于`on-policy`和`off-policy`的辨析：
> 
> 两者也被称为同策略和异策略（这种名词我感觉是比较能描述两者的区别的）
> 
> 笼统得来讲，在环境交互成本高，采集信息困难的场景下（如机器人强化学习，自动驾驶等），倾向于使用`off-policy`策略，异策略维护一个庞大的优先经验回放池，每次参数更新中随机从经验回放池中**随机抽取一定批次大小的数据**；
> 
> 在环境交互简单的场景下（如各种仿真场景等），倾向于使用`on-policy`策略，同策略实时维护一个较小的数据池（比如容量为2048），每次进行参数更新时，使用数据池中所有的数据更新多次，**然后清空经验池**，这种方式下数据**即采即弃**
>
> 两者的注意差别在于经验池的维护方式，注意之前理解的误区，两种策略训练时的数据流都是同时进行数据采集和参数更新的（效率高），我之前理解的是`off-policy`数据采集和参数更新是严格分开的（实际上不是且没有必要）

* 广义优势估计`GAE`

上面的优势估计函数中我们使用了单步的`TD`误差来估计优势函数：

$${\hat A_t} = {r_t} + \gamma {V_\pi }\left( {{s_{t + 1}}} \right) - {V_\pi }\left( {{s_t}} \right)$$

这样做的缺点是方差较大，收敛较慢，`GAE`通过引入一个衰减参数 $\lambda$ 来综合多步的`TD`误差，从而降低方差，提高估计的稳定性：

$${\delta _t} = {r_t} + \gamma V({s_{t + 1}}) - V({s_t})$$

$$\hat A_t^{GAE} = {\delta _t} + (\gamma \lambda )\hat A_{t + 1}^{GAE}$$

其中 $\lambda$ 为`GAE`的衰减系数，通常取值为0.95左右


## 2 `PPO`在`RLHF`中的使用

在经过前面一系列的学习后，`LLM`输出自然语言的过程实际是不断输出`next token`的自回归过程，模型的输出实际上可以看作一个马尔可夫过程，符合强化学习适用的场景

此时，`LLM`视为智能体，当前的所有`prompt`视为环境的状态，模型输出的一个个`token`视为动作，模型输出的`token`直接结合到上一个`prompt`跳转到环境的下一个状态（马尔可夫过程）

> SFT（有监督微调）的本质是模仿，容易受限于训练数据的平均质量并因死记硬背产生幻觉；而RLHF（人类反馈强化学习）的核心是对齐，它利用判别比生成容易的数据优势，通过奖励机制打破概率分布的平均数陷阱，教会模型权衡说什么才是人类想要的，从而实现比单纯模仿更强的泛化能力和逻辑上限。

因此我们自然想到在在大模型的微调时适用强化学习

![RLHF结构](https://raw.githubusercontent.com/Flower-Melon/image/main/img/2025/RLHF结构.png)

涉及四个模型：

* `Actor Model`：演员模型，这就是我们想要训练的目标语言模型
* `Critic Model`：评论家模型，`PPO`强化学习刚需价值网络
* `Reward Model`：奖励模型，这实际上就是强化学习中的奖励 $r$ ,因为`NLP`任务的奖励给出一个显式的表达式子是不可能的，这里只要使用一个网络进行替代
* `Reference Model`：参考模型，它的作用是在RLHF阶段给语言模型增加一些约束（`KL`散度），防止语言模型训歪

具体的过程详见这篇知乎帖子，因为我是初次阅读，见解还不够深刻，所以这一部分以帖子为主：[图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读](https://zhuanlan.zhihu.com/p/677607581)
